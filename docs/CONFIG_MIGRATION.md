# LLM é…ç½®ä¼˜åŒ–è¿ç§»è¯´æ˜

## å˜æ›´æ¦‚è¿°

æœ¬æ¬¡æ›´æ–°ä¼˜åŒ–äº† LLM API çš„é…ç½®æ–¹å¼ï¼Œé‡‡ç”¨**åŸºäºåœºæ™¯çš„æ¨¡å‹é…ç½®**ï¼Œä½¿å¾—ä¸åŒå·¥å…·å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒé…ç½®çš„ç»Ÿä¸€æ€§ã€‚

## ä¸»è¦å˜æ›´

### 1. ç¯å¢ƒå˜é‡é…ç½® (.env)

**ä¹‹å‰ï¼š**
```bash
OPENAI_API_KEY=sk-xxx
OPENAI_API_BASE=http://xxx/v1
OPENAI_MODEL=claude-sonnet-4-5-20250929
OPENAI_TEMPERATURE=0
VISION_MODEL=qwen-vl-max
```

**ç°åœ¨ï¼š**
```bash
# ç»Ÿä¸€çš„ API é…ç½®
OPENAI_API_KEY=sk-xxx
OPENAI_API_BASE=http://xxx/v1

# åœºæ™¯åŒ–æ¨¡å‹é…ç½®
DEFAULT_MODEL=claude-sonnet-4-5-20250929
CODE_GEN_MODEL=claude-sonnet-4-5-20250929
VISION_MODEL=qwen-vl-max
AGENT_MODEL=claude-sonnet-4-5-20250929

# åœºæ™¯å‚æ•°
CODE_GEN_TEMPERATURE=0.3
CODE_GEN_MAX_TOKENS=8192
VISION_TEMPERATURE=0
VISION_MAX_TOKENS=4096
```

### 2. Settings ç±» (config/settings.py)

**æ–°å¢å­—æ®µï¼š**
- `default_model` / `default_temperature`
- `code_gen_model` / `code_gen_temperature` / `code_gen_max_tokens`
- `vision_model` / `vision_temperature` / `vision_max_tokens`
- `agent_model` / `agent_temperature`

**å‘åå…¼å®¹ï¼š**
- `settings.openai_model` â†’ è‡ªåŠ¨æ˜ å°„åˆ° `settings.default_model`
- `settings.openai_temperature` â†’ è‡ªåŠ¨æ˜ å°„åˆ° `settings.default_temperature`

### 3. LLMClient ç±» (utils/llm_client.py)

**æ–°å¢æ–¹æ³•ï¼š**
```python
@classmethod
def for_scenario(cls, scenario: ScenarioType = "default") -> LLMClient:
    """æ ¹æ®åœºæ™¯åˆ›å»º LLM å®¢æˆ·ç«¯ï¼ˆæ¨èä½¿ç”¨ï¼‰"""
```

**æ”¯æŒçš„åœºæ™¯ï¼š**
- `"default"` - é»˜è®¤åœºæ™¯
- `"code_gen"` - ä»£ç ç”Ÿæˆåœºæ™¯
- `"vision"` - è§†è§‰ç†è§£åœºæ™¯
- `"agent"` - Agent åœºæ™¯

### 4. å·¥å…·æ›´æ–°

#### code_generator.py
**ä¹‹å‰ï¼š**
```python
llm_client = LLMClient.from_settings(settings)
response = llm_client.chat_completion(
    messages=[...],
    temperature=0.3,
    max_tokens=8192
)
```

**ç°åœ¨ï¼š**
```python
llm_client = LLMClient.for_scenario("code_gen")
response = llm_client.chat_completion(
    messages=[...],
    max_tokens=settings.code_gen_max_tokens if settings else int(os.getenv("CODE_GEN_MAX_TOKENS", "8192"))
)
```

#### visual_understanding.py
**ä¹‹å‰ï¼š**
```python
llm = LLMClient(model=model)
response = llm.vision_completion(
    prompt=prompt,
    image_data=image_data,
    max_tokens=4096
)
```

**ç°åœ¨ï¼š**
```python
llm = LLMClient.for_scenario("vision") if not model else LLMClient(model=model)
response = llm.vision_completion(
    prompt=prompt,
    image_data=image_data,
    max_tokens=int(os.getenv("VISION_MAX_TOKENS", "4096"))
)
```

## è¿ç§»æ­¥éª¤

### 1. æ›´æ–° .env æ–‡ä»¶

```bash
# å¤åˆ¶ç¤ºä¾‹é…ç½®
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„é…ç½®
# é‡ç‚¹å…³æ³¨ï¼š
# - OPENAI_API_KEY
# - OPENAI_API_BASE
# - å„åœºæ™¯çš„æ¨¡å‹é…ç½®
```

### 2. æ›´æ–°ä»£ç ï¼ˆå¦‚æœæœ‰è‡ªå®šä¹‰å·¥å…·ï¼‰

**æ¨èä½¿ç”¨åœºæ™¯åŒ–åˆ›å»ºï¼š**
```python
# æ—§æ–¹å¼
llm = LLMClient(model=os.getenv("OPENAI_MODEL"))

# æ–°æ–¹å¼ï¼ˆæ¨èï¼‰
llm = LLMClient.for_scenario("default")
```

### 3. æµ‹è¯•é…ç½®

```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python examples/test_llm_config.py
```

## ä¼˜åŠ¿

### 1. çµæ´»æ€§
- âœ… ä¸åŒåœºæ™¯ä½¿ç”¨ä¸åŒæ¨¡å‹
- âœ… åªéœ€ä¿®æ”¹ .env æ–‡ä»¶å³å¯åˆ‡æ¢æ¨¡å‹
- âœ… æ”¯æŒæ··åˆä½¿ç”¨å¤šä¸ªæ¨¡å‹

### 2. ç»Ÿä¸€æ€§
- âœ… æ‰€æœ‰æ¨¡å‹å…±ç”¨åŒä¸€ä¸ª API Key å’Œ Base URL
- âœ… é€‚åˆä½¿ç”¨ OpenAI ä¸­è½¬æœåŠ¡
- âœ… é…ç½®é›†ä¸­ç®¡ç†

### 3. å¯ç»´æŠ¤æ€§
- âœ… ä»£ç æ›´ç®€æ´ï¼Œå‡å°‘é‡å¤é…ç½®
- âœ… åœºæ™¯åŒ–å‘½åï¼Œè¯­ä¹‰æ¸…æ™°
- âœ… å‘åå…¼å®¹ï¼Œä¸å½±å“æ—§ä»£ç 

### 4. æˆæœ¬ä¼˜åŒ–
- âœ… ä¸ºä¸åŒåœºæ™¯é…ç½®ä¸åŒæ¨¡å‹
- âœ… åœ¨ä¿è¯æ•ˆæœçš„å‰æä¸‹é™ä½æˆæœ¬
- âœ… çµæ´»è°ƒæ•´å„åœºæ™¯çš„å‚æ•°

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šä½¿ç”¨ä¸­è½¬æœåŠ¡ï¼Œæ‰€æœ‰åœºæ™¯ç”¨ Claude

```bash
# .env
OPENAI_API_KEY=sk-xxx
OPENAI_API_BASE=http://your-proxy.com/v1

DEFAULT_MODEL=claude-sonnet-4-5-20250929
CODE_GEN_MODEL=claude-sonnet-4-5-20250929
VISION_MODEL=qwen-vl-max
AGENT_MODEL=claude-sonnet-4-5-20250929
```

### ç¤ºä¾‹ 2ï¼šæ··åˆä½¿ç”¨ä¸åŒæ¨¡å‹

```bash
# .env
OPENAI_API_KEY=sk-xxx
OPENAI_API_BASE=http://your-proxy.com/v1

DEFAULT_MODEL=gpt-4-turbo-preview
CODE_GEN_MODEL=claude-sonnet-4-5-20250929  # ä»£ç ç”Ÿæˆç”¨ Claude
VISION_MODEL=qwen-vl-max                    # è§†è§‰ç†è§£ç”¨ Qwen
AGENT_MODEL=gpt-4-turbo-preview             # Agent ç”¨ GPT-4
```

### ç¤ºä¾‹ 3ï¼šä»£ç ä¸­ä½¿ç”¨

```python
from utils.llm_client import LLMClient

# ä»£ç ç”Ÿæˆ
llm = LLMClient.for_scenario("code_gen")
code = llm.chat_completion(messages=[...])

# è§†è§‰ç†è§£
llm = LLMClient.for_scenario("vision")
result = llm.vision_completion(prompt="...", image_data="...")

# Agent
llm = LLMClient.for_scenario("agent")
response = llm.chat_completion(messages=[...])
```

## æ–‡æ¡£

- ğŸ“– [å®Œæ•´é…ç½®æŒ‡å—](./LLM_CONFIG_GUIDE.md)
- ğŸ§ª [æµ‹è¯•è„šæœ¬](../examples/test_llm_config.py)
- ğŸ“ [é…ç½®ç¤ºä¾‹](.env.example)

## å¸¸è§é—®é¢˜

**Q: æ—§ä»£ç ä¼šå—å½±å“å—ï¼Ÿ**  
A: ä¸ä¼šã€‚Settings ç±»æä¾›äº†å‘åå…¼å®¹çš„å±æ€§æ˜ å°„ã€‚

**Q: å¦‚ä½•å¿«é€Ÿåˆ‡æ¢æ¨¡å‹ï¼Ÿ**  
A: åªéœ€ä¿®æ”¹ .env æ–‡ä»¶ä¸­å¯¹åº”åœºæ™¯çš„ `*_MODEL` é…ç½®å³å¯ã€‚

**Q: å¯ä»¥ä¸ºæŸä¸ªå·¥å…·å•ç‹¬æŒ‡å®šæ¨¡å‹å—ï¼Ÿ**  
A: å¯ä»¥ã€‚åœ¨è°ƒç”¨å·¥å…·æ—¶ä¼ å…¥ `model` å‚æ•°å³å¯è¦†ç›–é»˜è®¤é…ç½®ã€‚

**Q: å¦‚ä½•éªŒè¯é…ç½®æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ**  
A: è¿è¡Œ `python examples/test_llm_config.py` æŸ¥çœ‹å½“å‰é…ç½®ã€‚

## åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– PRã€‚

