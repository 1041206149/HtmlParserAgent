"""
æµ‹è¯• LLM é…ç½®çš„ç¤ºä¾‹è„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒåœºæ™¯çš„ LLM å®¢æˆ·ç«¯
"""
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env")

from utils.llm_client import LLMClient
from config.settings import Settings


def test_scenario_based_clients():
    """æµ‹è¯•åŸºäºåœºæ™¯çš„å®¢æˆ·ç«¯åˆ›å»º"""
    print("=" * 70)
    print("æµ‹è¯•åœºæ™¯åŒ– LLM å®¢æˆ·ç«¯")
    print("=" * 70)
    
    scenarios = ["default", "code_gen", "vision", "agent"]
    
    for scenario in scenarios:
        print(f"\nğŸ“Œ åœºæ™¯: {scenario}")
        llm = LLMClient.for_scenario(scenario)
        print(f"   æ¨¡å‹: {llm.model}")
        print(f"   æ¸©åº¦: {llm.temperature}")
        print(f"   Base URL: {llm.api_base}")


def test_direct_initialization():
    """æµ‹è¯•ç›´æ¥åˆå§‹åŒ–"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç›´æ¥åˆå§‹åŒ–")
    print("=" * 70)
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    llm1 = LLMClient()
    print(f"\né»˜è®¤é…ç½®:")
    print(f"   æ¨¡å‹: {llm1.model}")
    print(f"   æ¸©åº¦: {llm1.temperature}")
    
    # æŒ‡å®šæ¨¡å‹
    llm2 = LLMClient(model="gpt-4-turbo-preview", temperature=0.7)
    print(f"\nè‡ªå®šä¹‰é…ç½®:")
    print(f"   æ¨¡å‹: {llm2.model}")
    print(f"   æ¸©åº¦: {llm2.temperature}")


def test_settings_based():
    """æµ‹è¯•ä» Settings åˆ›å»º"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•ä» Settings åˆ›å»º")
    print("=" * 70)
    
    settings = Settings()
    llm = LLMClient.from_settings(settings)
    print(f"\nä» Settings åˆ›å»º:")
    print(f"   æ¨¡å‹: {llm.model}")
    print(f"   æ¸©åº¦: {llm.temperature}")
    
    # è¦†ç›–æ¨¡å‹
    llm2 = LLMClient.from_settings(settings, model="custom-model")
    print(f"\nè¦†ç›–æ¨¡å‹:")
    print(f"   æ¨¡å‹: {llm2.model}")


def test_chat_completion():
    """æµ‹è¯•èŠå¤©å®Œæˆï¼ˆå¯é€‰ï¼Œéœ€è¦æœ‰æ•ˆçš„ API Keyï¼‰"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•èŠå¤©å®Œæˆ API è°ƒç”¨")
    print("=" * 70)
    
    if not os.getenv("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY") == "your_api_key_here":
        print("\nâš ï¸  è·³è¿‡ API è°ƒç”¨æµ‹è¯•ï¼ˆæœªé…ç½®æœ‰æ•ˆçš„ API Keyï¼‰")
        return
    
    try:
        llm = LLMClient.for_scenario("default")
        print(f"\nä½¿ç”¨æ¨¡å‹: {llm.model}")
        print("å‘é€æµ‹è¯•æ¶ˆæ¯...")
        
        response = llm.chat_completion(
            messages=[
                {"role": "user", "content": "Say 'Hello, World!' in one word."}
            ],
            max_tokens=10
        )
        
        print(f"âœ… å“åº”: {response}")
        
    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")


def show_current_config():
    """æ˜¾ç¤ºå½“å‰é…ç½®"""
    print("\n" + "=" * 70)
    print("å½“å‰ç¯å¢ƒé…ç½®")
    print("=" * 70)
    
    config_keys = [
        "OPENAI_API_KEY",
        "OPENAI_API_BASE",
        "DEFAULT_MODEL",
        "CODE_GEN_MODEL",
        "VISION_MODEL",
        "AGENT_MODEL",
    ]
    
    for key in config_keys:
        value = os.getenv(key, "æœªè®¾ç½®")
        # éšè— API Key
        if "KEY" in key and value != "æœªè®¾ç½®":
            value = value[:10] + "..." + value[-4:] if len(value) > 14 else "***"
        print(f"   {key}: {value}")


if __name__ == "__main__":
    print("\nğŸš€ LLM é…ç½®æµ‹è¯•è„šæœ¬\n")
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    show_current_config()
    
    # æµ‹è¯•ä¸åŒçš„åˆ›å»ºæ–¹å¼
    test_scenario_based_clients()
    test_direct_initialization()
    test_settings_based()
    
    # å¯é€‰ï¼šæµ‹è¯•å®é™… API è°ƒç”¨
    test_chat_completion()
    
    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 70)
    print("\nğŸ’¡ æç¤º:")
    print("   - ä¿®æ”¹ .env æ–‡ä»¶ä¸­çš„æ¨¡å‹é…ç½®")
    print("   - é‡æ–°è¿è¡Œæ­¤è„šæœ¬æŸ¥çœ‹å˜åŒ–")
    print("   - æŸ¥çœ‹ docs/LLM_CONFIG_GUIDE.md äº†è§£æ›´å¤š")
    print()

