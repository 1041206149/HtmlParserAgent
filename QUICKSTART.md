# å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ 5åˆ†é’Ÿä¸Šæ‰‹

### ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ–

```bash
# å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt
```

### ç¬¬äºŒæ­¥ï¼šéªŒè¯å®‰è£…

```bash
# è¿è¡Œè®¾ç½®æµ‹è¯•
python test_setup.py
```

å¦‚æœçœ‹åˆ° "ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!"ï¼Œè¯´æ˜å®‰è£…æˆåŠŸã€‚

### ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œç¤ºä¾‹

```bash
# è¿è¡Œç¤ºä¾‹ç¨‹åº
python example.py
```

è¿™å°†ï¼š
1. è·å–ç¤ºä¾‹URLçš„HTMLæºç 
2. æ•è·é¡µé¢æˆªå›¾
3. ä½¿ç”¨è§†è§‰æ¨¡å‹æå–é¡µé¢ç»“æ„
4. ç”ŸæˆBeautifulSoupè§£æä»£ç 
5. éªŒè¯ç”Ÿæˆçš„ä»£ç 

### ç¬¬å››æ­¥ï¼šæŸ¥çœ‹ç»“æœ

ç”Ÿæˆçš„æ–‡ä»¶ä½äº `output/` ç›®å½•ï¼š

```
output/
â”œâ”€â”€ blog/
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ generated_parser.py  # ç”Ÿæˆçš„è§£æå™¨ä»£ç 
â”‚   â”‚   â””â”€â”€ schema.json          # å­—æ®µé…ç½®
â”‚   â””â”€â”€ screenshots/
â”‚       â””â”€â”€ sample_1.png         # é¡µé¢æˆªå›¾
```

### ç¬¬äº”æ­¥ï¼šä½¿ç”¨ç”Ÿæˆçš„è§£æå™¨

```bash
# æ–¹å¼1: è§£æURL
python output/blog/parsers/generated_parser.py "https://example.com/article"

# æ–¹å¼2: è§£ææœ¬åœ°HTMLæ–‡ä»¶
python output/blog/parsers/generated_parser.py "path/to/file.html"
```

## ğŸ’¡ å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: è§£æåšå®¢æ–‡ç« 

```python
from agent import ParserAgent

agent = ParserAgent(output_dir="output/blog")

result = agent.generate_parser(
    urls=["https://blog.example.com/article-1"],
    layout_type="blog_article",
    validate=True
)

print(f"è§£æå™¨: {result['parser_path']}")
```

### åœºæ™¯2: è§£æç”µå•†äº§å“é¡µ

```python
from agent import ParserAgent

agent = ParserAgent(output_dir="output/ecommerce")

result = agent.generate_parser(
    urls=[
        "https://shop.example.com/product/123",
        "https://shop.example.com/product/456",
    ],
    layout_type="product_page",
    validate=True
)
```

### åœºæ™¯3: æ‰¹é‡å¤„ç†å¤šä¸ªURL

```python
from agent import ParserAgent

# è¯»å–URLåˆ—è¡¨
with open("urls.txt") as f:
    urls = [line.strip() for line in f if line.strip()]

agent = ParserAgent()
result = agent.generate_parser(urls=urls, validate=True)
```

### åœºæ™¯4: åˆ†æ­¥ä½¿ç”¨å·¥å…·

```python
from tools import (
    get_webpage_source,
    capture_webpage_screenshot,
    extract_json_from_image,
    generate_parser_code
)

url = "https://example.com"

# 1. è·å–HTML
html = get_webpage_source(url)

# 2. æˆªå›¾
screenshot = capture_webpage_screenshot(url)

# 3. æå–ç»“æ„
schema = extract_json_from_image(screenshot)

# 4. ç”Ÿæˆä»£ç 
result = generate_parser_code(html, schema)
```

## ğŸ”§ é…ç½®è°ƒæ•´

ç¼–è¾‘ `.env` æ–‡ä»¶æ¥è°ƒæ•´é…ç½®ï¼š

```bash
# è°ƒæ•´æœ€å¤§è¿­ä»£æ¬¡æ•°
MAX_ITERATIONS=10

# è°ƒæ•´éªŒè¯æˆåŠŸé˜ˆå€¼
SUCCESS_THRESHOLD=0.9

# è°ƒæ•´æœ€å°æ ·æœ¬æ•°é‡
MIN_SAMPLE_SIZE=3
```

## ğŸ“Š æŸ¥çœ‹æ—¥å¿—

æ—¥å¿—æ–‡ä»¶ä½äº `logs/` ç›®å½•ï¼š

```bash
# æŸ¥çœ‹ä»Šå¤©çš„æ—¥å¿—
tail -f logs/agent_$(date +%Y-%m-%d).log
```

## â“ å¸¸è§é—®é¢˜

### Q1: ç”Ÿæˆçš„ä»£ç ä¸èƒ½æ­£ç¡®è§£æï¼Ÿ

**A**: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
1. å¢åŠ æ ·æœ¬URLæ•°é‡ï¼ˆæä¾›2-3ä¸ªåŒç±»å‹URLï¼‰
2. æ‰‹åŠ¨è°ƒæ•´ç”Ÿæˆçš„ä»£ç 
3. æŸ¥çœ‹éªŒè¯æŠ¥å‘Šä¸­çš„é”™è¯¯ä¿¡æ¯

### Q2: å¦‚ä½•æé«˜è§£æå‡†ç¡®ç‡ï¼Ÿ

**A**: 
1. æä¾›æ›´å¤šæ ·æœ¬URL
2. ç¡®ä¿URLå±äºåŒä¸€å¸ƒå±€ç±»å‹
3. è°ƒæ•´ `SUCCESS_THRESHOLD` é…ç½®

### Q3: å¦‚ä½•å¤„ç†éœ€è¦ç™»å½•çš„é¡µé¢ï¼Ÿ

**A**: 
1. å…ˆæ‰‹åŠ¨è·å–HTMLå¹¶ä¿å­˜
2. ä½¿ç”¨ `generate_parser_code` ç›´æ¥ä»HTMLç”Ÿæˆä»£ç 

### Q4: ç”Ÿæˆçš„ä»£ç å¦‚ä½•é›†æˆåˆ°é¡¹ç›®ä¸­ï¼Ÿ

**A**:
1. å¤åˆ¶ `generated_parser.py` åˆ°ä½ çš„é¡¹ç›®
2. å¯¼å…¥ `WebPageParser` ç±»
3. è°ƒç”¨ `parse(html)` æ–¹æ³•

## ğŸ¯ ä¸‹ä¸€æ­¥

1. æŸ¥çœ‹ [README.md](README.md) äº†è§£è¯¦ç»†åŠŸèƒ½
2. æŸ¥çœ‹ [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) äº†è§£æ¶æ„è®¾è®¡
3. ä¿®æ”¹ `example.py` å°è¯•ä¸åŒçš„ä½¿ç”¨æ–¹å¼
4. æ ¹æ®éœ€æ±‚æ‰©å±•AgentåŠŸèƒ½

## ğŸ’¬ è·å–å¸®åŠ©

- æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶æ’æŸ¥é—®é¢˜
- è¿è¡Œ `python test_setup.py` æ£€æŸ¥ç¯å¢ƒ
- æŸ¥çœ‹ç”Ÿæˆä»£ç ä¸­çš„æ³¨é‡Š

